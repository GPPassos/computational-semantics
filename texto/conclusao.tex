
Neste trabalho, descrevemos o trabalho original e fizemos exercícios propostos em \cite{BlackburnBos:2005}, em particular, a integração com o vocabulário da Wordnet. O trabalho foi a nível de estudo, conhecendo e revisando a abordagem, não de produção de resultados originais. A abordagem destes autores é uma entre muitas. Por exemplo, a \teng{English Resource Grammar} de \citet{ERG}, uma gramática de ampla cobertura baseada em \teng{Head-Driven Phrase Structure Grammar} e no modelo semântico de \teng{Minimal Recursion Semantics}, foi aplicada para o problema de implicação textual, utilizando uma abordagem de alinhamento de grafos \citep{semantic-parsing}. Outras duas abordagens são a \teng{Grammatical Framework}, uma linguagem de programação funcional com o propósito de escrever gramáticas desenvolvida desde 1998, bem como a \teng{XLE}, um ambiente que auxilia a criação de \teng{Lexical Functional Grammars} \citep{xle}. \update

O que foi feito aqui é um ponto de partida para aplicações práticas, apesar de ainda não ser suficiente. Um desafio considerável para o que temos até o momento é a simplicidade da gramática. Apenas um conjunto muito restrito de frases é compreendido pelo nosso analisador sintático. Esse tipo de avanço já foi tentado. Por exemplo, em \cite{BosMarkert2006}, foi utilizado o modelo de gramática categorial combinatória (\teng{combinatory categorial grammar}, CCG), com um analisador sintático estatístico, enquanto \cite{Moot2010} propôs a plataforma Grail para gramáticas categoriais multimodais. Tais trabalhos também não utilizam lógica de primeira ordem, mas sim teoria de representação de discurso (\teng{discourse representation theory}, DRT), mais adequadas para superar algumas dificuldades de representação, como anáfora e tempos verbais \cite[p.~1]{Kamp:1993}.

O uso deste tipo de ferramenta, que foi explorado em um estágio inicial neste trabalho, parece se apresentar como interessante candidato para os problemas de análise profunda da semântica e da pragmática de linguagem natural.


\subsection{Trabalhos futuros}

  Como dissemos, a integração dos métodos estudados aqui com gramáticas mais sofisticadas é essencial para as aplicações práticas. Para isso, devem ser adaptados os métodos de extração da semântica, uma vez que a composicionalidade do modo que abordamos é fortemente dependente da estrutura sintática usada.
  
  Outra direção interessante é a sofisticação do sistema de diálogo. O Curt como apresentado por \citet{BlackburnBos:2005} é um protótipo simples. Para cada frase, ainda que seja achado um grande número de interpretações, cada uma filtrada por testes de consistência e informatividade, apenas uma é escolhida, sendo todas as outras descartadas. Esta é uma opção radical, mas soluciona o problema de explosão combinatória gerado por se acumular a todo momento todas as possibilidades. Um desenvolvimento interessante seria elaborar um critério que apontasse um ponto intermediário no \teng{trade-off} entre complexidade e memória da polissemia. %Isto pode ser combinado com algum procedimento de, no momento de leitura de uma frase, a limitação por interpretações semanticamente ``próximas''.
  
  Modos mais sistemáticos de avaliar a capacidade de cobertura do sistema também são necessários. Uma dificuldade de utilizar conjuntos de dados disponíveis é, como afirmamos, a simplicidade sintática atual. Uma possibilidade que não exploramos é testar em uma linguagem natural controlada, um subconjunto mais restrito de uma linguagem natural de modo a reduzir sua complexidade, como \texteng{Attempto Controlled English}. \citep{fuchs:reasoningweb2008}
  
  Por fim, um tema que motivou a escolha deste trabalho mas não chegou a ser abordado aqui é a aplicação destes métodos a domínios específicos. Em particular, acreditamos que estes métodos possam ser particularmente frutíferos na área de Direito, um campo em que muitos dos seus fenômenos são lingüísticos, discursivos e argumentativos, sendo de grande importância a semântica e as habilidades de inferência.